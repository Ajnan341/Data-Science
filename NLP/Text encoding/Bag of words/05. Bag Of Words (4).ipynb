{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885d05ca",
   "metadata": {},
   "source": [
    "# <span style = \"color:green; font-size:40px\"> Bag Of Words </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6312b2d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb88fde",
   "metadata": {},
   "source": [
    "To understand the bag of words approach, let's first start with the help of an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc775d",
   "metadata": {},
   "source": [
    "Suppose we have a corpus with three sentences:\n",
    "   * \"I like to play football\"\n",
    "   * \"Did you go outside to play tennis\"\n",
    "   * \"John and I play tennis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5d055",
   "metadata": {},
   "source": [
    "Now if we have to perform text classification, or any other task, on the above data using statistical techniques, we can not do so since statistical techniques work only with numbers. Therefore we need to convert these sentences into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3ddf4",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize the Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109a8e9",
   "metadata": {},
   "source": [
    "The first step in this regard is to convert the sentences in our corpus into tokens or individual words. Look at the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97c819",
   "metadata": {},
   "source": [
    "| Sentence 1 | Sentence 2 | Sentence 3 |\n",
    "| --- | --- | --- |\n",
    "| I | Did | John |\n",
    "| like | you | and |\n",
    "| to | go | I |\n",
    "| play | outside | play |\n",
    "| football | to | tennis |\n",
    "|  | play |  |\n",
    "|  | tennis |  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b27649",
   "metadata": {},
   "source": [
    "### Step 2: Create a Dictionary of Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319d38d",
   "metadata": {},
   "source": [
    "The next step is to create a dictionary that contains all the words in our corpus as keys and the frequency of the occurence of the words as values. In other words, we need to create a histogram of the words in our corpus. Look at the following table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca669bd5",
   "metadata": {},
   "source": [
    "| Word | Frequency |\n",
    "| --- | --- | \n",
    "| I | 2 | \n",
    "| like | 1 | \n",
    "| to | 2 | \n",
    "| play | 3 |\n",
    "| football | 1 |\n",
    "| Did | 1 | \n",
    "| you | 1 | \n",
    "| go | 1 |\n",
    "| outside | 1 |\n",
    "| tennis | 2 |\n",
    "| John | 1 |\n",
    "| and | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e41cde",
   "metadata": {},
   "source": [
    "In the table above, you can see each word in our corpus along with its frequency of occurence. For instance, you can see that since the word play occurs three times in the corpus (once in each sentence) its freqency is 3.\n",
    "\n",
    "In our corpus, we only had three sentences, therefore it is easy for us to create a dictionary that contains all the words. In real world scenarios, there will be millions of words in the dictionary. Some of the words will have very small frequency are not very useful, hence such words are removed. One way to remove the words with less frequency is to sort the word frequency dictionary in the decreasing order of the frequency and then filter the words having a frequency higher than a certain threshold.\n",
    "\n",
    "Let's sort our words frequency dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d9e4a",
   "metadata": {},
   "source": [
    "| Word | Frequency |\n",
    "| --- | --- | \n",
    "| play | 3 |\n",
    "|tennis | 2 |\n",
    "| to | 2 |\n",
    "| I | 2 |\n",
    "| football | 1 |\n",
    "| Did | 1 |\n",
    "| you | 1 |\n",
    "| go | 1 |\n",
    "| outside | 1 |\n",
    "| like | 1 |\n",
    "| John | 1 | \n",
    "| and | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71424f25",
   "metadata": {},
   "source": [
    "### Step 3: Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40fb5d",
   "metadata": {},
   "source": [
    "To create the bag of words model, we need to create a matrix where the columns correspond to the most frequent words in our dictionary where rows correspond to the document or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2feb0",
   "metadata": {},
   "source": [
    "Suppose we filter 8 most occuring words from our dictionary. Then the document frequency matrix will look like this:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e933f",
   "metadata": {},
   "source": [
    "|  | Play | Tennis | To | I | Football | Did | You | go |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Sentence 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| Sentence 2 | 1 | 1 | 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "| Sentence 3 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce40710",
   "metadata": {},
   "source": [
    "It is important to understand how the matrix is created. In the above matrix, the first row corresponds to the first sentence. In the first, the word \"play\" occurs once, therefore we added 1 in the first column. The word in the second column is \"Tennis\", it doesn't occur in the first sentence, therefore we added a 0 in the second column for sentence 1. Similarly, in the second sentence, both the words \"play\" and \"Tennis\" occur once, therefore we added 1 in the first two columns. However, in the fifth column, we added a 0, since the word \"Football\" doesn't occur in the second sentence. In this way, all the cells in the above matix are filled with either 0 or 1, depending upon the occurence of the word. Final corresponds to the bag of words model.\n",
    "\n",
    "In each row, you can see the numeric representation of the corresponding sentence. For instance, the first row shows the numeric representation of Sentence 1. This numeric representation can now be used as input to the statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e6ad0",
   "metadata": {},
   "source": [
    "### Bag of words model in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49f4ce",
   "metadata": {},
   "source": [
    "####  Let's import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8202d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4617b0",
   "metadata": {},
   "source": [
    "re(Regular Expression) will be used for some preprocessing task on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491947b1",
   "metadata": {},
   "source": [
    "The first thing we need to create our Bag of words model is a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1db573",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c878d81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f9c8e",
   "metadata": {},
   "source": [
    "The next step is to split the corpus into individual sentences. To do so, we will use the sent_tokenize function from the NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9194d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029b8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a2b39",
   "metadata": {},
   "source": [
    "Our text contains punctuations. We don't want punctuations to be the part of our word frequency dictionary. In the following script, we first convert our text into lower case and then will remove the punctuation from our text. Removing punctuation can result in multiple empty spaces. We will remove the empty spaces from the text using regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595cbb1",
   "metadata": {},
   "source": [
    "Look at the following script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418afe4a",
   "metadata": {},
   "source": [
    "The re module provides functions and support for regular expressions. re.sub() is used to replace substrings in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc1dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    corpus[i] = re.sub(r'\\W', ' ', corpus[i])\n",
    "    corpus[i] = re.sub(r'\\s+',' ', corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035e6e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data ',\n",
       " 'the goal is a computer capable of understanding the contents of documents including the contextual nuances of the language within them ',\n",
       " 'the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dc4d7",
   "metadata": {},
   "source": [
    "You can see that the text doesn't contain any special character or multiple empty spaces.\n",
    "\n",
    "Now we have our own corpus. The next step is tokenize the sentence in the corpus and create a dictionary that contains words and their corresponding frequencies in our corpus. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d266ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569e4b5",
   "metadata": {},
   "source": [
    "In the script above we created a dictionary called wordfreq. Next, we iterate through each sentence in the corpus. The sentence is tokenized into words. Next, we iterate through each word in the sentence. If the word doesn't exist in the wordfreq dictionary, we will add the word as the key and will set the value of word as 1. Otherwise, if word already exists in the dictionary, we will simply increment the key count by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d660c86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 2,\n",
       " 'language': 4,\n",
       " 'processing': 1,\n",
       " 'nlp': 1,\n",
       " 'is': 2,\n",
       " 'a': 2,\n",
       " 'subfield': 1,\n",
       " 'of': 5,\n",
       " 'linguistics': 1,\n",
       " 'computer': 2,\n",
       " 'science': 1,\n",
       " 'and': 5,\n",
       " 'artificial': 1,\n",
       " 'intelligence': 1,\n",
       " 'concerned': 1,\n",
       " 'with': 1,\n",
       " 'the': 8,\n",
       " 'interactions': 1,\n",
       " 'between': 1,\n",
       " 'computers': 2,\n",
       " 'human': 1,\n",
       " 'in': 2,\n",
       " 'particular': 1,\n",
       " 'how': 1,\n",
       " 'to': 2,\n",
       " 'program': 1,\n",
       " 'process': 1,\n",
       " 'analyze': 1,\n",
       " 'large': 1,\n",
       " 'amounts': 1,\n",
       " 'data': 1,\n",
       " 'goal': 1,\n",
       " 'capable': 1,\n",
       " 'understanding': 1,\n",
       " 'contents': 1,\n",
       " 'documents': 3,\n",
       " 'including': 1,\n",
       " 'contextual': 1,\n",
       " 'nuances': 1,\n",
       " 'within': 1,\n",
       " 'them': 1,\n",
       " 'technology': 1,\n",
       " 'can': 1,\n",
       " 'then': 1,\n",
       " 'accurately': 1,\n",
       " 'extract': 1,\n",
       " 'information': 1,\n",
       " 'insights': 1,\n",
       " 'contained': 1,\n",
       " 'as': 2,\n",
       " 'well': 1,\n",
       " 'categorize': 1,\n",
       " 'organize': 1,\n",
       " 'themselves': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5caba68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb9c30",
   "metadata": {},
   "source": [
    "Depending upon the task at hand, not all words are useful. In huge corpora, you can have millions of words. We can filter the most frequently occuring words. our corpus has 54 words in total. Let us filter down to the 20 most frequently occuring words. To do so, we can make use of Python's heap library. \n",
    "\n",
    "Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81632c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "most_freq = heapq.nlargest(20, wordfreq, key = wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34de00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'of',\n",
       " 'and',\n",
       " 'language',\n",
       " 'documents',\n",
       " 'natural',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'in',\n",
       " 'to',\n",
       " 'as',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'linguistics',\n",
       " 'science',\n",
       " 'artificial',\n",
       " 'intelligence']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4cf17",
   "metadata": {},
   "source": [
    "Now our most_freq list contains 20 most frequently occuring words along with their frequency of occurence.\n",
    "\n",
    "The final step is to convert the sentences in our corpus into their corresponding vector representation. The idea is straightforward, for each word in the most_freq dictionary if the word exists in the sentence, a 1 will be added for the word, else 0 will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e75731c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57570001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd7c29",
   "metadata": {},
   "source": [
    "In the script above we created an empty list sentence_vectors which will store vectors for all the sentences in the corpus. Next, we iterate through each sentence in the corpus and create an empty list sent_vec for the individual sentences. Similarly, we also tokenize the sentence. Next, we iterate through each word in the most_freq list and check if the word exists in the tokens for the sentence. If the word is a part of the sentence, 1 is appended to the individual sentence vector sent_vec, else 0 is appended. Finally, the sentence vector is added to the list sentense_vectors which contains vectors for all sentences. Basically, this sentence_vectors is our bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e96971",
   "metadata": {},
   "source": [
    "However, the bag of words model that we saw in the theory section was in the form of a matix. Our model is in the form of a list of lists. We can convert our model into matrix form using this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10cd9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55caa51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8601280",
   "metadata": {},
   "source": [
    "Basically, we converted our list into a two-dimensional numpy array using asarray function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa12187",
   "metadata": {},
   "source": [
    "### We can also use CountVectorizer model in Sklearn.feature_extraction.text to Create Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec578b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a414a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20473963",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be151c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 3 1 0 1 0 0 0 1 2 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 3 1 1 2 1 0 2\n",
      "  0 1 1 1 1 1 1 0 1 0 0 0 2 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 3\n",
      "  0 0 0 0 0 0 0 0 4 1 0 0 0 1 0 0 1]\n",
      " [1 0 0 2 0 2 0 1 0 1 0 0 0 1 0 0 0 2 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 1 3 0 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472641e",
   "metadata": {},
   "source": [
    "<b> Bag of words model is one of the three most commonly used word embedding approaches with TF-IDF and Word2Vec being the other two.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5719164",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
