{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) Theory\n",
    "Definition: The Bag of Words (BoW) is a simple and commonly used text representation technique in Natural Language Processing (NLP). It transforms text into a numerical representation, where each document is represented as a vector of word frequencies, ignoring grammar and word order but focusing on the occurrence of words.\n",
    "\n",
    "# Key Concepts:\n",
    "Corpus: A collection of text documents.\n",
    "\n",
    "Vocabulary: The set of unique words across all documents in the corpus.\n",
    "\n",
    "Word Frequency: The number of times each word appears in a document.\n",
    "\n",
    "# How BoW Works:\n",
    "Tokenization: Split the text into individual words (or tokens).\n",
    "\n",
    "Building the Vocabulary: Construct a list of all unique words from the entire corpus. This list forms the \"vocabulary.\"\n",
    "\n",
    "Encoding the Documents:For each document in the corpus, create a vector. The length of this vector is equal to the number of unique words (size of the vocabulary).\n",
    "\n",
    "Each position in the vector corresponds to a word from the vocabulary.\n",
    "\n",
    "The value in each position represents the frequency of the word in that document (i.e., how many times the word appears in the document).\n",
    "\n",
    "Example:\n",
    "    Letâ€™s say we have two documents in the corpus:\n",
    "\n",
    "    Document 1: \"I love NLP.\"\n",
    "    Document 2: \"NLP is awesome.\"\n",
    "    Step 1: Tokenization\n",
    "    Document 1: ['I', 'love', 'NLP']\n",
    "    Document 2: ['NLP', 'is', 'awesome']\n",
    "\n",
    "    Step 2: Build the Vocabulary\n",
    "    Vocabulary: ['I', 'love', 'NLP', 'is', 'awesome']\n",
    "\n",
    "    Step 3: Create Vectors\n",
    "    For Document 1:\n",
    "    [1, 1, 1, 0, 0] (1 occurrence of 'I', 1 occurrence of 'love', 1 occurrence of 'NLP', 0 occurrences of 'is', 0 occurrences of 'awesome')\n",
    "\n",
    "    For Document 2:\n",
    "    [0, 0, 1, 1, 1] (0 occurrences of 'I', 0 occurrences of 'love', 1 occurrence of 'NLP', 1 occurrence of 'is', 1 occurrence of 'awesome')\n",
    "\n",
    "# Advantages of BoW:\n",
    "Simplicity: Easy to understand and implement.\n",
    "\n",
    "Efficiency: Works well for many text classification tasks such as spam detection, sentiment analysis, etc.\n",
    "\n",
    "Interpretability: The representation is interpretable as it directly reflects word frequencies.\n",
    "\n",
    "# Limitations of BoW:\n",
    "\n",
    "Loss of Context: BoW ignores the order of words. For instance, \"I love NLP\" and \"NLP love I\" would be treated as the same vector.\n",
    "\n",
    "High Dimensionality: If the vocabulary is large, the vectors become large and sparse (most of the values in the vector are 0), making computation expensive.\n",
    "\n",
    "Semantic Information: BoW does not capture the meaning of words. Words like \"good\" and \"great\" would be treated as completely unrelated, even though they are semantically similar.\n",
    "\n",
    "# Improving BoW:\n",
    "N-grams: Instead of representing single words, consider word pairs (bigrams) or triples (trigrams) to capture some word order.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): A more informative variation of BoW that accounts for the importance of words by downscaling the impact of frequent words that appear in many documents.\n",
    "\n",
    "# Use Cases:\n",
    "Text Classification: Spam detection, sentiment analysis, topic classification.\n",
    "\n",
    "Information Retrieval: Document ranking based on relevance to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 2 1 3 1 1 1 2 3 2 1 1 1 1 1 2 2 1 1]]\n",
      "['and' 'area' 'artificial' 'as' 'between' 'challenges' 'computer'\n",
      " 'computers' 'concerned' 'derive' 'from' 'human' 'in' 'information'\n",
      " 'intelligence' 'interaction' 'interactions' 'involve' 'is' 'it'\n",
      " 'language' 'linguistics' 'many' 'meaning' 'natural' 'nlp' 'of'\n",
      " 'processing' 'related' 'science' 'subfield' 'such' 'the' 'to'\n",
      " 'understanding' 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language. As such, NLP is related to the area of \n",
    "human-computer interaction. Many challenges in NLP involve understanding natural language to derive meaning \n",
    "and information from it.\"\"\"\n",
    "\n",
    "\n",
    "# Apply Bag of Words encoding\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([text])\n",
    "\n",
    "# Display BoW result (word frequency vector)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language. As such, NLP is related to the area of \n",
    "human-computer interaction. Many challenges in NLP involve understanding natural language to derive meaning \n",
    "and information from it.\"\"\"\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform([text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1,\n",
       "        1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'area', 'artificial', 'as', 'between', 'challenges',\n",
       "       'computer', 'computers', 'concerned', 'derive', 'from', 'human',\n",
       "       'in', 'information', 'intelligence', 'interaction', 'interactions',\n",
       "       'involve', 'is', 'it', 'language', 'linguistics', 'many',\n",
       "       'meaning', 'natural', 'nlp', 'of', 'processing', 'related',\n",
       "       'science', 'subfield', 'such', 'the', 'to', 'understanding',\n",
       "       'with'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'content',\n",
       " 'encoding': 'utf-8',\n",
       " 'decode_error': 'strict',\n",
       " 'strip_accents': None,\n",
       " 'preprocessor': None,\n",
       " 'tokenizer': None,\n",
       " 'analyzer': 'word',\n",
       " 'lowercase': True,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'stop_words': None,\n",
       " 'max_df': 1.0,\n",
       " 'min_df': 1,\n",
       " 'max_features': None,\n",
       " 'ngram_range': (1, 1),\n",
       " 'vocabulary': None,\n",
       " 'binary': False,\n",
       " 'dtype': numpy.int64,\n",
       " 'fixed_vocabulary_': False,\n",
       " '_stop_words_id': 140721788710088,\n",
       " 'stop_words_': set(),\n",
       " 'vocabulary_': {'natural': 24,\n",
       "  'language': 20,\n",
       "  'processing': 27,\n",
       "  'nlp': 25,\n",
       "  'is': 18,\n",
       "  'subfield': 30,\n",
       "  'of': 26,\n",
       "  'linguistics': 21,\n",
       "  'computer': 6,\n",
       "  'science': 29,\n",
       "  'and': 0,\n",
       "  'artificial': 2,\n",
       "  'intelligence': 14,\n",
       "  'concerned': 8,\n",
       "  'with': 35,\n",
       "  'the': 32,\n",
       "  'interactions': 16,\n",
       "  'between': 4,\n",
       "  'computers': 7,\n",
       "  'human': 11,\n",
       "  'as': 3,\n",
       "  'such': 31,\n",
       "  'related': 28,\n",
       "  'to': 33,\n",
       "  'area': 1,\n",
       "  'interaction': 15,\n",
       "  'many': 22,\n",
       "  'challenges': 5,\n",
       "  'in': 12,\n",
       "  'involve': 17,\n",
       "  'understanding': 34,\n",
       "  'derive': 9,\n",
       "  'meaning': 23,\n",
       "  'information': 13,\n",
       "  'from': 10,\n",
       "  'it': 19},\n",
       " '_sklearn_version': '1.3.1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['natural', 'language', 'processing', 'nlp', 'is', 'subfield', 'of',\n",
       "        'linguistics', 'computer', 'science', 'and', 'artificial',\n",
       "        'intelligence', 'concerned', 'with', 'the', 'interactions',\n",
       "        'between', 'computers', 'human', 'as', 'such', 'related', 'to',\n",
       "        'area', 'interaction', 'many', 'challenges', 'in', 'involve',\n",
       "        'understanding', 'derive', 'meaning', 'information', 'from', 'it'],\n",
       "       dtype='<U13')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
