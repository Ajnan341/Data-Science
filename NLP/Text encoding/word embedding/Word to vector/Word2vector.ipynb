{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Word Embedding in NLP?\n",
    "Word Embedding is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meanings to have a similar representation.\n",
    "\n",
    "Word Embeddings are a method of extracting features out of text so that we can input those features into a machine learning model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words (BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a sparse matrix if most of the elements are zero. Large input vectors will mean a huge number of weights which will result in high computation required for training. Word Embeddings give a solution to these problems.\n",
    "\n",
    "# Need for Word Embedding?\n",
    "To reduce dimensionality\n",
    "To use a word to predict the words around it.\n",
    "Inter-word semantics must be captured.\n",
    "How are Word Embeddings used?\n",
    "They are used as input to machine learning models.\n",
    "Take the words —-> Give their numeric representation —-> Use in training or inference.\n",
    "To represent or visualize any underlying patterns of usage in the corpus that was used to train them.\n",
    "\n",
    "In a similar way, we can create word vectors for different words as well on the basis of given features. The words with similar vectors are most likely to have the same meaning or are used to convey the same sentiment.\n",
    "\n",
    "# Approaches for Text Representation\n",
    "# 1. Traditional Approach\n",
    "The conventional method involves compiling a list of distinct terms and giving each one a unique integer value, or id. and after that, insert each word’s distinct id into the sentence. Every vocabulary word is handled as a feature in this instance. Thus, a large vocabulary will result in an extremely large feature size. Common traditional methods include:\n",
    "\n",
    "# 1.1. One-Hot Encoding\n",
    "One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1.\n",
    "While one-hot encoding is a simple and intuitive method for representing words in NLP, it has several disadvantages, which may limit its effectiveness in certain applications.\n",
    "\n",
    "One-hot encoding results in high-dimensional vectors, making it computationally expensive and memory-intensive, especially with large vocabularies.\n",
    "It does not capture semantic relationships between words; each word is treated as an isolated entity without considering its meaning or context.\n",
    "It is restricted to the vocabulary seen during training, making it unsuitable for handling out-of-vocabulary words.\n",
    "\n",
    "# 1.2. Bag of Word (Bow)\n",
    "Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation.\n",
    "\n",
    "While BoW is a simple and interpretable representation, below disadvantages highlight its limitations in capturing certain aspects of language structure and semantics:\n",
    "\n",
    "BoW ignores the order of words in the document, leading to a loss of sequential information and context making it less effective for tasks where word order is crucial, such as in natural language understanding.\n",
    "BoW representations are often sparse, with many elements being zero resulting in increased memory requirements and computational inefficiency, especially when dealing with large datasets.\n",
    "# 1.3. Term frequency-inverse document frequency (TF-IDF)\n",
    "Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:\n",
    "\n",
    "Term Frequency (TF): Term Frequency measures how often a term (word) appears in a document.           \n",
    "\n",
    "Inverse Document Frequency (IDF): Inverse Document Frequency measures the importance of a term across a collection of documents. \n",
    "The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:\n",
    "\n",
    "TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)          \n",
    "\n",
    "The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering.\n",
    "\n",
    "Let’s Implement Term Frequency-Inverse Document Frequency (TF-IDF) using python with the scikit-learn library. It begins by defining a set of sample documents. The TfidfVectorizer is employed to transform these documents into a TF-IDF matrix. The code then extracts and prints the TF-IDF values for each word in each document. This statistical measure helps assess the importance of words in a document relative to their frequency across a collection of documents, aiding in information retrieval and text analysis tasks.\n",
    "\n",
    "TF-IDF is a widely used technique in information retrieval and text mining, but its limitations should be considered, especially when dealing with tasks that require a deeper understanding of language semantics. For example:\n",
    "\n",
    "TF-IDF treats words as independent entities and doesn’t consider semantic relationships between them. This limitation hinders its ability to capture contextual information and word meanings.\n",
    "Sensitivity to Document Length: Longer documents tend to have higher overall term frequencies, potentially biasing TF-IDF towards longer documents.\n",
    "# 2. Neural Approach\n",
    "## 2.1. Word2Vec\n",
    "Word2Vec is a neural approach for generating word embeddings. It belongs to the family of neural word embedding techniques and specifically falls under the category of distributed representation models. It is a popular technique in natural language processing (NLP) that is used to represent words as continuous vector spaces. Developed by a team at Google, Word2Vec aims to capture the semantic relationships between words by mapping them to high-dimensional vectors. The underlying idea is that words with similar meanings should have similar vector representations. In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.6228903e-03  3.6640612e-03  5.1995856e-03  5.7334779e-03\n",
      "  7.4570905e-03 -6.1744191e-03  1.0984222e-03  6.0582189e-03\n",
      " -2.8444759e-03 -6.1877300e-03 -4.1608122e-04 -8.3801262e-03\n",
      " -5.6023225e-03  7.1134185e-03  3.3506860e-03  7.2196117e-03\n",
      "  6.8130088e-03  7.5271088e-03 -3.7921285e-03 -5.7889870e-04\n",
      "  2.3492652e-03 -4.5099277e-03  8.3968453e-03 -9.8600173e-03\n",
      "  6.7578158e-03  2.9302840e-03 -4.9444796e-03  4.3994249e-03\n",
      " -1.7387009e-03  6.7221164e-03  9.9649020e-03 -4.3673944e-03\n",
      " -6.0853740e-04 -5.7161008e-03  3.8461862e-03  2.8021245e-03\n",
      "  6.9052088e-03  6.0953684e-03  9.5311170e-03  9.2763416e-03\n",
      "  7.9116691e-03 -6.9992831e-03 -9.1550248e-03 -3.6017137e-04\n",
      " -3.0979610e-03  7.8838654e-03  5.9249173e-03 -1.5498387e-03\n",
      "  1.5103049e-03  1.7842182e-03  7.8201881e-03 -9.5230276e-03\n",
      " -2.0481556e-04  3.4776574e-03 -9.4633829e-04  8.3850771e-03\n",
      "  9.0182032e-03  6.5341769e-03 -7.1102567e-04  7.7052424e-03\n",
      " -8.5333847e-03  3.1948513e-03 -4.6267984e-03 -5.0804727e-03\n",
      "  3.5976472e-03  5.3858552e-03  7.7671749e-03 -5.7720132e-03\n",
      "  7.4189943e-03  6.6271950e-03 -3.7110827e-03 -8.7341517e-03\n",
      "  5.4375269e-03  6.5078773e-03 -7.7919790e-04 -6.7069679e-03\n",
      " -7.0896079e-03 -2.4907547e-03  5.1424033e-03 -3.6575736e-03\n",
      " -9.3804430e-03  3.8252671e-03  4.8847292e-03 -6.4260215e-03\n",
      "  1.1996601e-03 -2.0738926e-03  2.7704644e-05 -9.8690307e-03\n",
      "  2.6886624e-03 -4.7386698e-03  1.0951957e-03 -1.5719489e-03\n",
      "  2.1932647e-03 -7.8898817e-03 -2.7109219e-03  2.6668212e-03\n",
      "  5.3474940e-03 -2.3938406e-03 -9.5161078e-03  4.5168912e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language. As such, NLP is related to the area of \n",
    "human-computer interaction. Many challenges in NLP involve understanding natural language to derive meaning \n",
    "and information from it.\"\"\"\n",
    "\n",
    "\n",
    "# Tokenize the text into sentences, then words\n",
    "sentences = [text.split()]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access vector representation of a word\n",
    "word_vector = model.wv['language']\n",
    "\n",
    "# Display word vector\n",
    "print(word_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two neural embedding methods for Word2Vec, Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "# 2.2. Continuous Bag of Words(CBOW)\n",
    "\n",
    "Continuous Bag of Words (CBOW) is a type of neural network architecture used in the Word2Vec model. The primary objective of CBOW is to predict a target word based on its context, which consists of the surrounding words in a given window. Given a sequence of words in a context window, the model is trained to predict the target word at the center of the window.\n",
    "\n",
    "CBOW is a feedforward neural network with a single hidden layer. The input layer represents the context words, and the output layer represents the target word. The hidden layer contains the learned continuous vector representations (word embeddings) of the input words.\n",
    "\n",
    "The architecture is useful for learning distributed representations of words in a continuous vector space.\n",
    "\n",
    "\n",
    "\n",
    "The hidden layer contains the continuous vector representations (word embeddings) of the input words.\n",
    "\n",
    "The weights between the input layer and the hidden layer are learned during training.\n",
    "The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Word Vector for 'cat': [-0.01631721  0.0089922  -0.00827343  0.00164993  0.01699845 -0.00892532\n",
      "  0.00903522 -0.01357311 -0.00709744  0.01879622 -0.00315539  0.00064151\n",
      " -0.00828214 -0.01536442 -0.00301554  0.00494069 -0.00177506  0.01106851\n",
      " -0.00548655  0.00452008  0.010912    0.01669134 -0.00290626 -0.01841786\n",
      "  0.00874216  0.00114401  0.01488318 -0.00162592 -0.00527712 -0.01750513\n",
      " -0.00171165  0.00565252  0.01080284  0.01410456 -0.01140574  0.00371808\n",
      "  0.01217882 -0.00959526 -0.00621315  0.01359672  0.00326414  0.0003788\n",
      "  0.00694596  0.00043551  0.01923731  0.01012244 -0.01783401 -0.01408344\n",
      "  0.00180315  0.01278541]\n",
      "\n",
      "Most similar words to 'cat': [('bird', 0.12493900209665298), ('barked', 0.07400304824113846), ('the', 0.042383477091789246), ('at', 0.018274817615747452), ('over', 0.011328201740980148), ('on', 0.0013540179934352636), ('mat', -0.11909283697605133), ('flew', -0.1742609143257141), ('dog', -0.17549626529216766), ('sat', -0.24705801904201508)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n",
    "    ['the', 'bird', 'flew', 'over', 'the', 'cat']\n",
    "]\n",
    "\n",
    "# Train CBOW model (cbow means sg=0)\n",
    "cbow_model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=0)\n",
    "\n",
    "# Access the word vector for 'cat'\n",
    "word_vector = cbow_model.wv['cat']\n",
    "print(\"CBOW Word Vector for 'cat':\", word_vector)\n",
    "\n",
    "# Finding most similar words to 'cat'\n",
    "similar_words = cbow_model.wv.most_similar('cat')\n",
    "print(\"\\nMost similar words to 'cat':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Explanation of Parameters:\n",
    "    vector_size=50: Dimensionality of the word vectors.\n",
    "    window=3: Context window size (words around the target word to be considered).\n",
    "    min_count=1: Ignores words with total frequency lower than this.\n",
    "    sg=0: 0 means the CBOW architecture (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0\n",
      "Epoch 2, Loss: 0\n",
      "Epoch 3, Loss: 0\n",
      "Epoch 4, Loss: 0\n",
      "Epoch 5, Loss: 0\n",
      "Epoch 6, Loss: 0\n",
      "Epoch 7, Loss: 0\n",
      "Epoch 8, Loss: 0\n",
      "Epoch 9, Loss: 0\n",
      "Epoch 10, Loss: 0\n",
      "Epoch 11, Loss: 0\n",
      "Epoch 12, Loss: 0\n",
      "Epoch 13, Loss: 0\n",
      "Epoch 14, Loss: 0\n",
      "Epoch 15, Loss: 0\n",
      "Epoch 16, Loss: 0\n",
      "Epoch 17, Loss: 0\n",
      "Epoch 18, Loss: 0\n",
      "Epoch 19, Loss: 0\n",
      "Epoch 20, Loss: 0\n",
      "Epoch 21, Loss: 0\n",
      "Epoch 22, Loss: 0\n",
      "Epoch 23, Loss: 0\n",
      "Epoch 24, Loss: 0\n",
      "Epoch 25, Loss: 0\n",
      "Epoch 26, Loss: 0\n",
      "Epoch 27, Loss: 0\n",
      "Epoch 28, Loss: 0\n",
      "Epoch 29, Loss: 0\n",
      "Epoch 30, Loss: 0\n",
      "Epoch 31, Loss: 0\n",
      "Epoch 32, Loss: 0\n",
      "Epoch 33, Loss: 0\n",
      "Epoch 34, Loss: 0\n",
      "Epoch 35, Loss: 0\n",
      "Epoch 36, Loss: 0\n",
      "Epoch 37, Loss: 0\n",
      "Epoch 38, Loss: 0\n",
      "Epoch 39, Loss: 0\n",
      "Epoch 40, Loss: 0\n",
      "Epoch 41, Loss: 0\n",
      "Epoch 42, Loss: 0\n",
      "Epoch 43, Loss: 0\n",
      "Epoch 44, Loss: 0\n",
      "Epoch 45, Loss: 0\n",
      "Epoch 46, Loss: 0\n",
      "Epoch 47, Loss: 0\n",
      "Epoch 48, Loss: 0\n",
      "Epoch 49, Loss: 0\n",
      "Epoch 50, Loss: 0\n",
      "Epoch 51, Loss: 0\n",
      "Epoch 52, Loss: 0\n",
      "Epoch 53, Loss: 0\n",
      "Epoch 54, Loss: 0\n",
      "Epoch 55, Loss: 0\n",
      "Epoch 56, Loss: 0\n",
      "Epoch 57, Loss: 0\n",
      "Epoch 58, Loss: 0\n",
      "Epoch 59, Loss: 0\n",
      "Epoch 60, Loss: 0\n",
      "Epoch 61, Loss: 0\n",
      "Epoch 62, Loss: 0\n",
      "Epoch 63, Loss: 0\n",
      "Epoch 64, Loss: 0\n",
      "Epoch 65, Loss: 0\n",
      "Epoch 66, Loss: 0\n",
      "Epoch 67, Loss: 0\n",
      "Epoch 68, Loss: 0\n",
      "Epoch 69, Loss: 0\n",
      "Epoch 70, Loss: 0\n",
      "Epoch 71, Loss: 0\n",
      "Epoch 72, Loss: 0\n",
      "Epoch 73, Loss: 0\n",
      "Epoch 74, Loss: 0\n",
      "Epoch 75, Loss: 0\n",
      "Epoch 76, Loss: 0\n",
      "Epoch 77, Loss: 0\n",
      "Epoch 78, Loss: 0\n",
      "Epoch 79, Loss: 0\n",
      "Epoch 80, Loss: 0\n",
      "Epoch 81, Loss: 0\n",
      "Epoch 82, Loss: 0\n",
      "Epoch 83, Loss: 0\n",
      "Epoch 84, Loss: 0\n",
      "Epoch 85, Loss: 0\n",
      "Epoch 86, Loss: 0\n",
      "Epoch 87, Loss: 0\n",
      "Epoch 88, Loss: 0\n",
      "Epoch 89, Loss: 0\n",
      "Epoch 90, Loss: 0\n",
      "Epoch 91, Loss: 0\n",
      "Epoch 92, Loss: 0\n",
      "Epoch 93, Loss: 0\n",
      "Epoch 94, Loss: 0\n",
      "Epoch 95, Loss: 0\n",
      "Epoch 96, Loss: 0\n",
      "Epoch 97, Loss: 0\n",
      "Epoch 98, Loss: 0\n",
      "Epoch 99, Loss: 0\n",
      "Epoch 100, Loss: 0\n",
      "Embedding for 'embeddings': [[ 0.0063935  -1.1651729   0.15446532  1.944693    1.1442418   1.2635216\n",
      "  -1.312425   -0.46490186  0.81631285  2.1001549 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define CBOW model\n",
    "class CBOWModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size):\n",
    "\t\tsuper(CBOWModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\t\tself.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "\tdef forward(self, context):\n",
    "\t\tcontext_embeds = self.embeddings(context).sum(dim=1)\n",
    "\t\toutput = self.linear(context_embeds)\n",
    "\t\treturn output\n",
    "\n",
    "# Sample data\n",
    "context_size = 2\n",
    "raw_text = \"word embeddings are awesome\"\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(tokens) - 2):\n",
    "\tcontext = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]]\n",
    "\ttarget = word_to_index[tokens[i]]\n",
    "\tdata.append((torch.tensor(context), torch.tensor(target)))\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize CBOW model\n",
    "cbow_model = CBOWModel(vocab_size, embed_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\ttotal_loss = 0\n",
    "\tfor context, target in data:\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutput = cbow_model(context)\n",
    "\t\tloss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item()\n",
    "\tprint(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage: Get embedding for a specific word\n",
    "word_to_lookup = \"embeddings\"\n",
    "word_index = word_to_index[word_to_lookup]\n",
    "embedding = cbow_model.embeddings(torch.tensor([word_index]))\n",
    "print(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Skip-Gram\n",
    "The Skip-Gram model learns distributed representations of words in a continuous vector space. The main objective of Skip-Gram is to predict context words (words surrounding a target word) given a target word. This is the opposite of the Continuous Bag of Words (CBOW) model, where the objective is to predict the target word based on its context. It is shown that this method produces more meaningful embeddings.\n",
    "\n",
    "\n",
    "\n",
    "After applying the above neural embedding methods we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.\n",
    "\n",
    "Let’s understand with a basic example. The python code contains, vector_size parameter that controls the dimensionality of the word vectors, and you can adjust other parameters such as window based on your specific needs.\n",
    "\n",
    "Note: Word2Vec models can perform better with larger datasets. If you have a large corpus, you might achieve more meaningful word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Word Vector for 'cat': [-0.01631985  0.0089937  -0.00827527  0.00164995  0.01700127 -0.00892669\n",
      "  0.00903687 -0.01357599 -0.00709863  0.01880009 -0.00315596  0.00064206\n",
      " -0.00828341 -0.01536769 -0.00301628  0.00494125 -0.00177575  0.01107021\n",
      " -0.00548739  0.00452096  0.01091394  0.01669473 -0.00290724 -0.01842082\n",
      "  0.00874346  0.00114407  0.01488624 -0.00162646 -0.00527802 -0.01750879\n",
      " -0.00171249  0.00565381  0.01080491  0.01410751 -0.0114081   0.00371863\n",
      "  0.01218077 -0.00959739 -0.00621481  0.0135988   0.00326435  0.00037924\n",
      "  0.00694775  0.00043561  0.01924111  0.01012394 -0.01783769 -0.01408602\n",
      "  0.00180341  0.01278773]\n",
      "\n",
      "Most similar words to 'cat': [('bird', 0.12490319460630417), ('barked', 0.07400049269199371), ('the', 0.04237981140613556), ('at', 0.01827564835548401), ('over', 0.011206768453121185), ('on', 0.001355123007670045), ('mat', -0.11909693479537964), ('flew', -0.17425644397735596), ('dog', -0.17548997700214386), ('sat', -0.24706168472766876)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Same sample sentences for training\n",
    "sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n",
    "    ['the', 'bird', 'flew', 'over', 'the', 'cat']\n",
    "]\n",
    "\n",
    "# Train Skip-Gram model (skip-gram means sg=1)\n",
    "skipgram_model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Access the word vector for 'cat'\n",
    "word_vector = skipgram_model.wv['cat']\n",
    "print(\"Skip-Gram Word Vector for 'cat':\", word_vector)\n",
    "\n",
    "# Finding most similar words to 'cat'\n",
    "similar_words = skipgram_model.wv.most_similar('cat')\n",
    "print(\"\\nMost similar words to 'cat':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Explanation of Parameters:\n",
    "    sg=1: 1 means the Skip-Gram architecture.\n",
    "    The rest of the parameters remain the same as in the CBOW example.\n",
    "    Key Differences:\n",
    "    CBOW (in gensim): sg=0\n",
    "    Predicts a word given its surrounding context (the words before and after the target word).\n",
    "    Skip-Gram (in gensim): sg=1\n",
    "    Predicts the context words given a target word.\n",
    "    Outputs:\n",
    "    Both examples will print the word vector for \"cat\" and show words that are most similar to \"cat\" based on the trained embeddings. The results from CBOW and Skip-Gram may differ slightly as CBOW tends to work better for frequent words, while Skip-Gram works better for infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'word': [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03\n",
      "  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03\n",
      " -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03\n",
      "  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04\n",
      " -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03\n",
      "  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03\n",
      "  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03\n",
      "  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03\n",
      "  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03\n",
      " -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03\n",
      " -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03\n",
      " -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03\n",
      " -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04\n",
      " -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03\n",
      "  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03\n",
      " -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03\n",
      " -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03\n",
      " -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03\n",
      "  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03\n",
      " -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03\n",
      " -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03\n",
      "  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03\n",
      " -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03\n",
      "  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03\n",
      "  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # Download the tokenizer models if not already downloaded\n",
    "\n",
    "sample = \"Word embeddings are dense vector representations of words.\"\n",
    "tokenized_corpus = word_tokenize(sample.lower()) # Lowercasing for consistency\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus],\n",
    "\t\t\t\t\t\tvector_size=100, # Dimensionality of the word vectors\n",
    "\t\t\t\t\t\twindow=5,\t\t # Maximum distance between the current and predicted word within a sentence\n",
    "\t\t\t\t\t\tsg=1,\t\t\t # Skip-Gram model (1 for Skip-Gram, 0 for CBOW)\n",
    "\t\t\t\t\t\tmin_count=1,\t # Ignores all words with a total frequency lower than this\n",
    "\t\t\t\t\t\tworkers=4)\t # Number of CPU cores to use for training the model\n",
    "\n",
    "# Training\n",
    "skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\n",
    "skipgram_model.save(\"skipgram_model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "vector_representation = loaded_model.wv['word']\n",
    "print(\"Vector representation of 'word':\", vector_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
