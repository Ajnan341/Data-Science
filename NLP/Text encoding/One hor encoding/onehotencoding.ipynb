{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding:\n",
    "One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector’s size equivalent to the number of potential categories. \n",
    "\n",
    "For example, if we have a feature with three categories (A, B, and C), \n",
    "each category can be represented as a binary vector of length three, \n",
    "with the vector for category A being [1, 0, 0], \n",
    "the vector for category B being [0, 1, 0], and the vector for category C being [0, 0, 1].\n",
    "# Why One-Hot Encoding is Used in NLP:\n",
    "One-hot encoding is used in NLP to encode categorical factors as binary vectors, such as words or part-of-speech identifiers. \n",
    "This approach is helpful because machine learning algorithms generally act on numerical data, so representing text data as numerical vectors are required for these algorithms to work.\n",
    "In a sentiment analysis assignment, for example, we might describe each word in a sentence as a one-hot encoded vector and then use these vectors as input to a neural network to forecast the sentiment of the sentence.\n",
    "Example 1:\n",
    "Suppose we have a small corpus of text that contains three sentences:\n",
    "\n",
    "The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "She sells seashells by the seashore.\n",
    "\n",
    "Peter Piper picked a peck of pickled peppers.\n",
    "\n",
    "Each word in these phrases should be represented as a single compressed vector. The first stage is to determine the categorical variable, which is the phrases’ terms. The second stage is to count the number of distinct words in the sentences to calculate the number of potential groups. In this instance, there are 17 potential categories.\n",
    "\n",
    "The third stage is to make a binary vector for each of the categories. Because there are 17 potential groups, each binary vector will be 17 bytes long. For example, the binary vector for the word “quick” will be [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], with the 1s in the first and sixth places because “quick” is both the first and sixth group in the list of unique words.\n",
    "\n",
    "Finally, we use the binary vectors generated in step 3 to symbolize each word in the sentences as a one-hot encoded vector. For example, the one-hot encoded vector for the word “quick” in the first sentence is [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], and the one-hot encoded vector for the word “seashells” in the second sentence is [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0].\n",
    "\n",
    "# Python Implementation for One-Hot Encoding in NLP\n",
    "Now let’s try to implement the above example using Python. Because finally, we will have to perform this programmatically else it won’t be possible for us to use this technique to train NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors for the first sentence:\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the corpus of text\n",
    "corpus = [\n",
    "\t\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\"She sells seashells by the seashore.\",\n",
    "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
    "]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "\tfor word in sentence.split():\n",
    "\t\tunique_words.add(word.lower())\n",
    "\n",
    "# Create a dictionary to map each\n",
    "# unique word to an index\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "\tword_to_index[word] = i\n",
    "\n",
    "# Create one-hot encoded vectors for\n",
    "# each word in the corpus\n",
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "\tsentence_vectors = []\n",
    "\tfor word in sentence.split():\n",
    "\t\tvector = np.zeros(len(unique_words))\n",
    "\t\tvector[word_to_index[word.lower()]] = 1\n",
    "\t\tsentence_vectors.append(vector)\n",
    "\tone_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors \n",
    "# for the first sentence\n",
    "print(\"One-hot encoded vectors for the first sentence:\")\n",
    "for vector in one_hot_vectors[0]:\n",
    "\tprint(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample text as a list of words\n",
    "words = text.split()\n",
    "\n",
    "# Convert words to a 2D array for one-hot encoding\n",
    "unique_words = np.array(words).reshape(-1, 1)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = encoder.fit_transform(unique_words)\n",
    "\n",
    "# Display One-Hot Encoded result\n",
    "print(one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Natural']\n",
      " ['language']\n",
      " ['processing']\n",
      " ['(NLP)']\n",
      " ['is']\n",
      " ['a']\n",
      " ['subfield']\n",
      " ['of']\n",
      " ['linguistics,']\n",
      " ['computer']\n",
      " ['science,']\n",
      " ['and']\n",
      " ['artificial']\n",
      " ['intelligence']\n",
      " ['concerned']\n",
      " ['with']\n",
      " ['the']\n",
      " ['interactions']\n",
      " ['between']\n",
      " ['computers']\n",
      " ['and']\n",
      " ['human']\n",
      " ['language.']\n",
      " ['As']\n",
      " ['such,']\n",
      " ['NLP']\n",
      " ['is']\n",
      " ['related']\n",
      " ['to']\n",
      " ['the']\n",
      " ['area']\n",
      " ['of']\n",
      " ['human-computer']\n",
      " ['interaction.']\n",
      " ['Many']\n",
      " ['challenges']\n",
      " ['in']\n",
      " ['NLP']\n",
      " ['involve']\n",
      " ['understanding']\n",
      " ['natural']\n",
      " ['language']\n",
      " ['to']\n",
      " ['derive']\n",
      " ['meaning']\n",
      " ['and']\n",
      " ['information']\n",
      " ['from']\n",
      " ['it.']]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language. As such, NLP is related to the area of \n",
    "human-computer interaction. Many challenges in NLP involve understanding natural language to derive meaning \n",
    "and information from it.\"\"\"\n",
    "\n",
    "\n",
    "w=text.split()\n",
    "unique_words=np.array(w).reshape(-1,1)\n",
    "print(unique_words)\n",
    "encoder= OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded=encoder.fit_transform(unique_words)\n",
    "\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
