{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffd1baf",
   "metadata": {},
   "source": [
    "# <span style = \"color:green\"> What is Tokenization?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae60fd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aed300",
   "metadata": {},
   "source": [
    "Tokenization is one of the most common tasks when it comes to working with text data. But what does the term 'tokenization' actually mean?\n",
    "\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "\n",
    "Let's Visualize it with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a87ba9",
   "metadata": {},
   "source": [
    "<center><div class=\"alert alert-block alert-warning\">\n",
    "<b>Natural Language Processing</b>\n",
    "</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa889d",
   "metadata": {},
   "source": [
    "<center><div class=\"alert alert-block alert-success\">\n",
    "<b>['Natural','Language','Processing']</b>\n",
    "</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f1af0",
   "metadata": {},
   "source": [
    "The tokens could be words, numbers or punctuation marks. In tokenization, smaller units are creaeted by locating word boundaries. Wait- what are word boundaries?\n",
    "\n",
    "These are the ending point of a word and the beginnning of the next word. These tokens are considered as a first step for stemming and lemmatization(We will discuss about these later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacfa7c",
   "metadata": {},
   "source": [
    "### Why is Tokenization required in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f443353",
   "metadata": {},
   "source": [
    "I want you to think about the English language here. Pick up any sentence you can think of and hold that in your mind as you read this section. This will help you understand the importance of tokenization in a much easier manner.\n",
    "\n",
    "Before preprocessing a natural language, we need to identify the words that constitute a string of characters. That's why tokenization is the most basic step to proceed with NLP (text data). This is important because the meaning of the text could easily be interpreted by analysing the words present in the text.\n",
    "\n",
    "Let's take an example. Consider the below string:\n",
    "<center>\"This is a cat\"</center>\n",
    "\n",
    "What do you think will happen after we perform tokenization on this string? We get\n",
    "<center>['This','is','a','cat']</center>\n",
    "\n",
    "There are numerous uses of doing this. We can use this tokenized form to :\n",
    "* Count the number of words in the text.\n",
    "* Count the frequency of the word, that is, the number of times a particular word is present.\n",
    "\n",
    "And so on. We can extract alot more information which we'll discuss in detail in future articles. For now, it's time to dive into the meat of this article-the different methods of performing tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90885bf",
   "metadata": {},
   "source": [
    "## Methods to perform Tokenization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9819118",
   "metadata": {},
   "source": [
    "We are going to look at different ways we can perform tokenization on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bd730",
   "metadata": {},
   "source": [
    "### 1. Tokenization using python's split() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89f6fd",
   "metadata": {},
   "source": [
    "#### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeff8229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002,',\n",
       " 'SpaceX’s',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi-planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self-sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'Mars.',\n",
       " 'In',\n",
       " '2008,',\n",
       " 'SpaceX’s',\n",
       " 'Falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid-fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'Earth.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "# Splits at space \n",
    "text.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a02b33",
   "metadata": {},
   "source": [
    "#### Sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea4cbf",
   "metadata": {},
   "source": [
    "This is similar to word tokenization. Here, we study the structure of sentence in the analysis. A sentence usually ends with a full stop(.), So we can use\".\" as a seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d01a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars',\n",
       " 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "# Splits at '.' \n",
    "text.split('. ') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab44cc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "One major drawback of using python's split() method is that we can use only one seperator at a time. Another thing to note-in word tokenization, split() did not consider punctuation as a seperate token.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fcb748",
   "metadata": {},
   "source": [
    "### 2. Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259eae2",
   "metadata": {},
   "source": [
    "Now, this is a library you will appreciate the more you work with text data, NLTK, short for Natual Language Toolkit, is a library written in Python for symbolic and statistical Natural Language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873ce5a",
   "metadata": {},
   "source": [
    "NLTK contains a module called tokenize() which further classifies into two sub-categories:\n",
    "* <b>Word Tokenize:</b> We use the word_tokenize() method to split a sentence into tokens or words\n",
    "* <b>Sentence Tokenize:</b> We use the sent_tokenize() method to split a document or paragraph into sentences\n",
    "\n",
    "Let's see both of these done one-by-one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0336106",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e609bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’',\n",
       " 's',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi-planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self-sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'Mars',\n",
       " '.',\n",
       " 'In',\n",
       " '2008',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’',\n",
       " 's',\n",
       " 'Falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid-fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a329ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0b7e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One of the first steps in cleaning data for NLP is to remove unwanted characters from your text, such as punctuation, numbers, symbols, HTML tags, or emojis.', 'These characters can introduce noise and ambiguity to your data, and may not be relevant for your NLP task.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "t=\"One of the first steps in cleaning data for NLP is to remove unwanted characters from your text, such as punctuation, numbers, symbols, HTML tags, or emojis. These characters can introduce noise and ambiguity to your data, and may not be relevant for your NLP task.\"\n",
    "st=sent_tokenize(t)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bbc441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['One', 'of', 'the', 'first', 'steps', 'in', 'cleaning', 'data', 'for', 'NLP', 'is', 'to', 'remove', 'unwanted', 'characters', 'from', 'your', 'text', ',', 'such', 'as', 'punctuation', ',', 'numbers', ',', 'symbols', ',', 'HTML', 'tags', ',', 'or', 'emojis', '.'], ['These', 'characters', 'can', 'introduce', 'noise', 'and', 'ambiguity', 'to', 'your', 'data', ',', 'and', 'may', 'not', 'be', 'relevant', 'for', 'your', 'NLP', 'task', '.']]\n"
     ]
    }
   ],
   "source": [
    "w_t=[word_tokenize(i) for i in st]\n",
    "print(w_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e4e6d",
   "metadata": {},
   "source": [
    "Notice how NLTK is considering punctuation as token? Hence for future tasks, we need to remove the punctuations from the initial list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8e211",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140d25f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.',\n",
       " 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07afa2d",
   "metadata": {},
   "source": [
    "### 3. Tokenization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7190e56",
   "metadata": {},
   "source": [
    "Keras! One of the hottest deep learning frameworks in the industry right now. It is an open source neural network library for Python. Keras is super easy to use and can also run on top of tensorflow.\n",
    "\n",
    "In the NLP context, we can use Keras for cleaning the unstructured text data that we typically collect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a0568",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06465439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['founded',\n",
       " 'in',\n",
       " '2002',\n",
       " 'spacex’s',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi',\n",
       " 'planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self',\n",
       " 'sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'mars',\n",
       " 'in',\n",
       " '2008',\n",
       " 'spacex’s',\n",
       " 'falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid',\n",
       " 'fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'earth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# define\n",
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "# tokenize\n",
    "result = text_to_word_sequence(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9593102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'of', 'the', 'first', 'steps', 'in', 'cleaning', 'data', 'for', 'nlp', 'is', 'to', 'remove', 'unwanted', 'characters', 'from', 'your', 'text', 'such', 'as', 'punctuation', 'numbers', 'symbols', 'html', 'tags', 'or', 'emojis', 'these', 'characters', 'can', 'introduce', 'noise', 'and', 'ambiguity', 'to', 'your', 'data', 'and', 'may', 'not', 'be', 'relevant', 'for', 'your', 'nlp', 'task']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "t=\"One of the first steps in cleaning data for NLP is to remove unwanted characters from your text, such as punctuation, numbers, symbols, HTML tags, or emojis. These characters can introduce noise and ambiguity to your data, and may not be relevant for your NLP task.\"\n",
    "print(text_to_word_sequence(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a054da",
   "metadata": {},
   "source": [
    "Keras lowers the case of all the alphabets before tokenizing them. That saves us quite a lot of time as you can imagine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a815c1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a50fff",
   "metadata": {},
   "source": [
    "Tokenization is a critical step in the overall NLP Pipeline. We cannot simply jump into the model building part without cleaning the text first.\n",
    "\n",
    "There are various other ways as well to tokenize but these ones we discussed are good enough to get started on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2c5e9",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
